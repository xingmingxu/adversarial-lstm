{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Adversarial LSTM\n",
        "\n",
        "Implementing Adversarial LSTM, using a TensorFlow computational graph, as described in \"Enhancing Stock Movement Prediction with Adversarial Training\".\n",
        "\n",
        "Imports follow..."
      ],
      "metadata": {
        "id": "NDoA_PT4pzp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow\n",
        "! pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhKfO_GBoz0r",
        "outputId": "ca4c009d-21b0-4922-8346-9cc28a5a6011"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from time import time\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "IsHHBaWVpzZk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Data\n",
        "\n"
      ],
      "metadata": {
        "id": "2wjugAx5qmwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The authors of this paper used two datasets -- ACL18 & KDD17 -- as benchmarked in prior research conducted in this field. This notebook uses the KDD17 dataset, which is stored as:\n",
        "\n",
        "> * kdd17\n",
        "  *   tickers\n",
        "   *   AMZ.csv (etc.)\n",
        "  *   training_dates.csv\n",
        "\n",
        "The loaded data is temporally split into training/validation/testing datasets, and each split into three further components: the present value (pv), a weekday indicator vector (wd), and the ground truth (gt).\n",
        "\n",
        "Instances (timeframes) without enough history, which can be defined by **seq**, are dropped.\n",
        "\n"
      ],
      "metadata": {
        "id": "SL8n-8hXqve2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_temp(data_path, tra_date, val_date, tes_date,\n",
        "                   seq=2, date_format='%Y-%m-%d'):\n",
        "  file_names = [f for f in os.listdir(data_path)\n",
        "            if os.path.isfile(os.path.join(data_path, f))]\n",
        "  print(file_names)\n",
        "\n",
        "  data_EOD = []\n",
        "\n",
        "  # Obtain all file names under the data path\n",
        "  for index, fname in enumerate(file_names):\n",
        "    single_EOD = np.genfromtxt(\n",
        "        os.path.join(data_path, fname), dtype=np.float64, delimiter=',',\n",
        "        skip_header=False\n",
        "    )\n",
        "    data_EOD.append(single_EOD)\n",
        "  fea_dim = data_EOD[0].shape[1] - 2\n",
        "\n",
        "  trading_dates = np.genfromtxt(\n",
        "    'kdd17/trading_dates.csv', dtype=str,\n",
        "    delimiter=',', skip_header=False\n",
        "  )\n",
        "\n",
        "  # One-hot encode the weekdays of training dates\n",
        "  dates_index = {}\n",
        "  data_wd = np.zeros([len(trading_dates), 5], dtype=np.float64)\n",
        "  wd_encodings = np.identity(5, dtype=np.float64)\n",
        "  for index, date in enumerate(trading_dates):\n",
        "      dates_index[date] = index\n",
        "      data_wd[index] = wd_encodings[datetime.strptime(date, date_format).weekday()]\n",
        "\n",
        "  # Separate into tra/val/tes instances\n",
        "  tra_ind = dates_index[tra_date]\n",
        "  val_ind = dates_index[val_date]\n",
        "  tes_ind = dates_index[tes_date]\n",
        "\n",
        "  # count training, validation, and testing instances\n",
        "  tra_num = 0\n",
        "  val_num = 0\n",
        "  tes_num = 0\n",
        "\n",
        "  # training\n",
        "  for date_ind in range(tra_ind, val_ind):\n",
        "    # filter out instances without length enough history\n",
        "    if date_ind < seq:\n",
        "      continue\n",
        "    for tic_ind in range(len(file_names)):\n",
        "      if abs(data_EOD[tic_ind][date_ind][-2]) > 1e-8:\n",
        "        if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "          tra_num += 1\n",
        "  #print(tra_num, ' training instances')\n",
        "\n",
        "  # validation\n",
        "  for date_ind in range(val_ind, tes_ind):\n",
        "    # filter out instances without length enough history\n",
        "    if date_ind < seq:\n",
        "      continue\n",
        "    for tic_ind in range(len(file_names)):\n",
        "      if abs(data_EOD[tic_ind][date_ind][-2]) > 1e-8:\n",
        "        if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "          val_num += 1\n",
        "  #print(val_num, ' validation instances')\n",
        "\n",
        "  # testing\n",
        "  for date_ind in range(tes_ind, len(trading_dates)):\n",
        "    # filter out instances without length enough history\n",
        "    if date_ind < seq:\n",
        "      continue\n",
        "    for tic_ind in range(len(file_names)):\n",
        "      if abs(data_EOD[tic_ind][date_ind][-2]) > 1e-8:\n",
        "        if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "          tes_num += 1\n",
        "  #print(tes_num, ' testing instances')\n",
        "\n",
        "\n",
        "  # generate training, validation, and testing instances\n",
        "  # training\n",
        "  tra_pv = np.zeros([tra_num, seq, fea_dim], dtype=np.float64)\n",
        "  tra_wd = np.zeros([tra_num, seq, 5], dtype=np.float64)\n",
        "  tra_gt = np.zeros([tra_num, 1], dtype=np.float64)\n",
        "  ins_ind = 0\n",
        "\n",
        "  for date_ind in range(tra_ind, val_ind):\n",
        "    # filter out instances without length enough history\n",
        "    if date_ind < seq:\n",
        "      continue\n",
        "    for tic_ind in range(len(file_names)):\n",
        "        if abs(data_EOD[tic_ind][date_ind][-2]) > 1e-8 and \\\n",
        "                data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "          tra_pv[ins_ind] = data_EOD[tic_ind][date_ind - seq: date_ind, : -2]\n",
        "          tra_wd[ins_ind] = data_wd[date_ind - seq: date_ind, :]\n",
        "          tra_gt[ins_ind, 0] = (data_EOD[tic_ind][date_ind][-2] + 1) / 2\n",
        "          ins_ind += 1\n",
        "\n",
        "  # validation\n",
        "  val_pv = np.zeros([val_num, seq, fea_dim], dtype=np.float64)\n",
        "  val_wd = np.zeros([val_num, seq, 5], dtype=np.float64)\n",
        "  val_gt = np.zeros([val_num, 1], dtype=np.float64)\n",
        "  ins_ind = 0\n",
        "\n",
        "  for date_ind in range(val_ind, tes_ind):\n",
        "    # filter out instances without length enough history\n",
        "    if date_ind < seq:\n",
        "      continue\n",
        "    for tic_ind in range(len(file_names)):\n",
        "      if abs(data_EOD[tic_ind][date_ind][-2]) > 1e-8 and \\\n",
        "                      data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "        val_pv[ins_ind] = data_EOD[tic_ind][date_ind - seq: date_ind, :-2]\n",
        "        val_wd[ins_ind] = data_wd[date_ind - seq: date_ind, :]\n",
        "        val_gt[ins_ind, 0] = (data_EOD[tic_ind][date_ind][-2] + 1) / 2\n",
        "        ins_ind += 1\n",
        "\n",
        "  # testing\n",
        "  tes_pv = np.zeros([tes_num, seq, fea_dim], dtype=np.float64)\n",
        "  tes_wd = np.zeros([tes_num, seq, 5], dtype=np.float64)\n",
        "  tes_gt = np.zeros([tes_num, 1], dtype=np.float64)\n",
        "  ins_ind = 0\n",
        "\n",
        "  for date_ind in range(tes_ind, len(trading_dates)):\n",
        "    # filter out instances without length enough history\n",
        "    if date_ind < seq:\n",
        "      continue\n",
        "    for tic_ind in range(len(file_names)):\n",
        "      if abs(data_EOD[tic_ind][date_ind][-2]) > 1e-8 and \\\n",
        "                        data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "        tes_pv[ins_ind] = data_EOD[tic_ind][date_ind - seq: date_ind, :-2]\n",
        "        # # for the momentum indicator\n",
        "        # tes_pv[ins_ind, -1, -1] = data_EOD[tic_ind][date_ind - 1, -1] - data_EOD[tic_ind][date_ind - 11, -1]\n",
        "        tes_wd[ins_ind] = data_wd[date_ind - seq: date_ind, :]\n",
        "        tes_gt[ins_ind, 0] = (data_EOD[tic_ind][date_ind][-2] + 1) / 2\n",
        "        ins_ind += 1\n",
        "\n",
        "  # pv: present_value, wd: weekday, gt: ground_truth\n",
        "  return tra_pv, tra_wd, tra_gt, val_pv, val_wd, val_gt, tes_pv, tes_wd, tes_gt\n"
      ],
      "metadata": {
        "id": "QTnOL4xNqWuP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ALSTM class"
      ],
      "metadata": {
        "id": "okJNO4XzvxeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path=\"kdd17/tickers\"\n",
        "tra_date='2014-01-02'\n",
        "val_date='2015-08-03'\n",
        "tes_date='2015-10-01'\n",
        "seq=5\n",
        "date_format='%Y-%m-%d'"
      ],
      "metadata": {
        "id": "bGs2oJ-R1BpD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE_NAME = '/gpu:0'\n",
        "tf.random.set_seed(20241029)\n",
        "print('device name:', DEVICE_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRxQCeql8PfC",
        "outputId": "0eb666d7-6189-44fb-cd7d-fe9a1308908c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device name: /gpu:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following block contains the ALSTM class.\n",
        "\n",
        "This AttentionLayer is a wrapper class for the attention layer in the AdvLSTM class.\n",
        "\n",
        "AdvLSTM is the main class for the model."
      ],
      "metadata": {
        "id": "gecTj_nE3b7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(layers.Layer):\n",
        "\n",
        "  \"\"\"\n",
        "  A custom Layer to process the attention mechanism described by the authors.\n",
        "  As the Keras computational graph requires wrappers around tensors, due to its\n",
        "  incompatability with eager execution, the corresponding layers have been\n",
        "  moved into here.\n",
        "\n",
        "  This layer computes a custom attention \"score\" for the LSTM output.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, units, **kwargs):\n",
        "      super(AttentionLayer, self).__init__(**kwargs)\n",
        "      # Set up the required variables for processing, formerly part of the\n",
        "      # AdvLSTM class.\n",
        "      self.units = units\n",
        "      self.av_W = tf.Variable(initial_value=glorot_uniform()(shape=(units, units)), trainable=True)\n",
        "      self.av_b = tf.Variable(initial_value=tf.zeros((units,)), trainable=True)\n",
        "      self.av_u = tf.Variable(initial_value=glorot_uniform()(shape=(units,)), trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      a_laten = tf.tanh(tf.tensordot(inputs, self.av_W, axes=1) + self.av_b)\n",
        "      a_scores = tf.tensordot(a_laten, self.av_u, axes=1) # Dot product the scores\n",
        "      a_alphas = tf.nn.softmax(a_scores, axis=1)  # Apply softmax along time axis\n",
        "\n",
        "      # Obtain context vector\n",
        "      a_con = tf.reduce_sum(inputs * tf.expand_dims(a_alphas, -1), 1)\n",
        "      return a_con\n",
        "\n",
        "  # This is needed for saving and loading the model with custom layers\n",
        "  def get_config(self):\n",
        "      config = super(AttentionLayer, self).get_config()\n",
        "      config.update({'units': self.units})\n",
        "      return config"
      ],
      "metadata": {
        "id": "LnCHKamCvAnP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvLSTM():\n",
        "\n",
        "  \"\"\"\n",
        "  AdvLSTM class.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data_path, model_path, save_path,\n",
        "                parameters,\n",
        "                steps=1,\n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                gpu=True,\n",
        "                tra_date='2014-01-02', val_date='2015-08-03', tes_date='2015-10-01',\n",
        "                date_format='%Y-%m-%d',\n",
        "                att=0, hinge=0, fix_init=0, adv=0, reload=0):\n",
        "\n",
        "    self.data_path = data_path\n",
        "    self.model_path = model_path\n",
        "    self.save_path = save_path\n",
        "\n",
        "    self.parameters = parameters\n",
        "    self.steps = steps\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.tra_date = tra_date\n",
        "    self.val_date = val_date\n",
        "    self.tes_date = tes_date\n",
        "    self.date_format = date_format\n",
        "\n",
        "    self.att = att\n",
        "    self.hinge = hinge\n",
        "    self.fix_init = fix_init\n",
        "    self.adv = adv\n",
        "    self.reload = reload\n",
        "\n",
        "    self.fea_dim = None\n",
        "\n",
        "    # Load data\n",
        "    self.tra_pv, self.tra_wd, self.tra_gt, \\\n",
        "    self.val_pv, self.val_wd, self.val_gt, \\\n",
        "    self.tes_pv, self.tes_wd, self.tes_gt = load_data_temp(\n",
        "      data_path=self.data_path,\n",
        "      tra_date=self.tra_date,\n",
        "      val_date=self.val_date,\n",
        "      tes_date=self.tes_date,\n",
        "      seq=self.parameters['seq'],\n",
        "      date_format=self.date_format\n",
        "    )\n",
        "    self.fea_dim = self.tra_pv.shape[2]  # Number of features\n",
        "    self.model = self.construct_model()\n",
        "\n",
        "    self.attention_score_layer = layers.Dense(self.parameters['unit'], activation='tanh')  # Create Dense layer here\n",
        "    self.attention_weights_layer = layers.Dense(1, activation='softmax') # Create Dense layer here\n",
        "    self.model = self.construct_model()\n",
        "\n",
        "\n",
        "  def get_batch(self, sta_ind=None):\n",
        "\n",
        "    \"\"\"\n",
        "    Standard-issue batching.\n",
        "    \"\"\"\n",
        "\n",
        "    if sta_ind is None:\n",
        "      sta_ind = random.randrange(0, self.tra_pv.shape[0])\n",
        "    if sta_ind + self.batch_size < self.tra_pv.shape[0]:\n",
        "      end_ind = sta_ind + self.batch_size\n",
        "    else:\n",
        "      sta_ind = self.tra_pv.shape[0] - self.batch_size\n",
        "      end_ind = self.tra_pv.shape[0]\n",
        "    return (\n",
        "      self.tra_pv[sta_ind:end_ind, :, :],\n",
        "      self.tra_wd[sta_ind:end_ind, :, :],\n",
        "      self.tra_gt[sta_ind:end_ind, :]\n",
        "    )\n",
        "\n",
        "  def compute_loss(self, pred, gt, hinge_loss=False):\n",
        "\n",
        "    \"\"\"\n",
        "    Computes the loss for the model. If I remember correctly, the authors prefer\n",
        "    Hinge loss.\n",
        "    \"\"\"\n",
        "\n",
        "    if hinge_loss:\n",
        "      return tf.reduce_mean(tf.maximum(0.0, 1 - gt * pred))  # Hinge loss for binary classification\n",
        "    else:\n",
        "      return tf.keras.losses.BinaryCrossentropy()(gt, pred)  # Cross-entropy for probabilities\n",
        "\n",
        "  def construct_model(self):\n",
        "\n",
        "    \"\"\"\n",
        "    Builds the Keras model for the adversarial LSTM.\n",
        "    - Incorporates the Dense + LSTM layers, as well as custom AttentionLayer\n",
        "    - If this is successful, the model is compiled and returned.\n",
        "    - Sets up the graph but does not train the model.\n",
        "    - Adversarial examples are generated outside of it. (tbd if valid)\n",
        "    \"\"\"\n",
        "\n",
        "    pv_input = layers.Input(shape=(self.parameters['seq'], self.fea_dim))\n",
        "    wd_input = layers.Input(shape=(self.parameters['seq'], 5))\n",
        "    gt_input = layers.Input(shape=(1,))\n",
        "\n",
        "    units = self.parameters['unit']\n",
        "\n",
        "    # LSTM and Attention Layer\n",
        "    dense_layer = layers.TimeDistributed(layers.Dense(self.fea_dim, activation='tanh'))(pv_input)\n",
        "    lstm_output = layers.LSTM(units, return_sequences=True)(dense_layer)\n",
        "    a_con = AttentionLayer(units)(lstm_output) # Obtain results of the attention layer.\n",
        "\n",
        "    # Extract the last timestep's LSTM output\n",
        "    lstm_last_step = layers.Lambda(lambda x: x[:, -1, :])(lstm_output)\n",
        "\n",
        "    # Concatenate features\n",
        "    feature_concat = layers.Concatenate()([lstm_last_step, a_con])\n",
        "\n",
        "    # Prediction layer\n",
        "    predictions = layers.Dense(1, activation='linear')(feature_concat)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[pv_input, wd_input, gt_input], outputs=predictions)\n",
        "    return model\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "\n",
        "    \"\"\"\n",
        "    Trains the entire model.\n",
        "    \"\"\"\n",
        "\n",
        "    self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.parameters['lr']),\n",
        "                       loss=self.compute_loss,\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "      for batch in range(self.tra_pv.shape[0] // self.batch_size):\n",
        "        pv_b, wd_b, gt_b = self.get_batch(batch * self.batch_size)\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:  # Tape must be persistent. this is a strange nuance.\n",
        "          predictions = self.model([pv_b, wd_b, gt_b], training=True) # First generate predictions.\n",
        "          loss = self.compute_loss(predictions, gt_b, hinge_loss=self.hinge)\n",
        "\n",
        "          # Compute adversarial loss if enabled\n",
        "          if self.adv:\n",
        "            tape.watch(predictions)\n",
        "            adv_loss = self.compute_loss(predictions, gt_b, hinge_loss=self.hinge)\n",
        "            # Calculate the gradients.\n",
        "            delta_adv = tape.gradient(adv_loss, predictions)\n",
        "            delta_adv = tf.stop_gradient(delta_adv)\n",
        "            delta_adv = tf.nn.l2_normalize(delta_adv, axis=1)\n",
        "            perturbed_inputs = predictions + self.parameters['eps'] * delta_adv\n",
        "\n",
        "            adv_loss = self.compute_loss(perturbed_inputs, gt_b, hinge_loss=self.hinge)\n",
        "\n",
        "        # Compute gradients for normal training\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "        # Compute gradients for adversarial loss (if enabled)\n",
        "        if self.adv:\n",
        "          adv_gradients = tape.gradient(adv_loss, self.model.trainable_variables)\n",
        "          self.model.optimizer.apply_gradients(zip(adv_gradients, self.model.trainable_variables))\n",
        "\n",
        "        del tape  # free memory\n",
        "\n",
        "      # Evaluate on validation data\n",
        "      val_perf = self.model.evaluate([self.val_pv, self.val_wd, self.val_gt], self.val_gt, verbose=0)\n",
        "      print(f\"Epoch {epoch + 1}/{self.epochs}, Validation Loss: {val_perf[0]}, Accuracy: {val_perf[1]}\")\n"
      ],
      "metadata": {
        "id": "iH9Wxhl9vxFp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for your model, adjust based on your setup\n",
        "parameters = {\n",
        "    'seq': 5,  # Sequence length\n",
        "    'unit': 64,  # Number of units in LSTM\n",
        "    'lr': 0.001,  # Learning rate\n",
        "    'eps': 0.01,  # Epsilon for adversarial perturbations\n",
        "    'alp': 0.1,   # Regularization term\n",
        "    'bet': 0.1,   # Adversarial loss weight\n",
        "    # Add any other parameters required by your model\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize your model\n",
        "tttttttt = AdvLSTM(\n",
        "    data_path=data_path,\n",
        "    model_path=None,\n",
        "    save_path=None,\n",
        "    parameters=parameters,\n",
        "    steps=1,  # Example: steps for processing\n",
        "    epochs=50,\n",
        "    batch_size=256,\n",
        "    gpu=False,\n",
        "    tra_date='2014-01-02',\n",
        "    val_date='2015-08-03',\n",
        "    tes_date='2015-10-01',\n",
        "    date_format='%Y-%m-%d',\n",
        "    att=0, hinge=0, fix_init=0, reload=0,\n",
        "    adv=0, # enable adversarial training\n",
        ")\n",
        "\n",
        "# Just a simple demonstration to ensure that this works.\n",
        "model = tttttttt.construct_model()\n",
        "predictions = model.predict(x=[tttttttt.tes_pv, tttttttt.tes_wd, tttttttt.tes_gt])\n",
        "print(predictions.shape)  # Should print (num_test_samples, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwsbFVcOavmM",
        "outputId": "cd513ddd-87d3-4d2e-e28a-d78341190c34"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['DCM.csv', 'BAC.csv', 'BA.csv', 'CHL.csv', 'CMCSA.csv', 'CVX.csv', 'AAPL.csv', 'D.csv', 'BRK-B.csv', 'BHP.csv', 'AMZN.csv']\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "(2268, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model and obtain results."
      ],
      "metadata": {
        "id": "D6K5D2aKyOb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tttttttt.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K005dPlxpm2T",
        "outputId": "92aa2d1b-5941-44d7-b848-1ed62b34d11b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Validation Loss: 7.451866149902344, Accuracy: 0.5692771077156067\n",
            "Epoch 2/50, Validation Loss: 7.736703872680664, Accuracy: 0.5271084308624268\n",
            "Epoch 3/50, Validation Loss: 7.860823631286621, Accuracy: 0.5030120611190796\n",
            "Epoch 4/50, Validation Loss: 7.907081604003906, Accuracy: 0.4969879388809204\n",
            "Epoch 5/50, Validation Loss: 7.912779808044434, Accuracy: 0.4939759075641632\n",
            "Epoch 6/50, Validation Loss: 7.904788970947266, Accuracy: 0.5512048006057739\n",
            "Epoch 7/50, Validation Loss: 7.896721363067627, Accuracy: 0.5240963697433472\n",
            "Epoch 8/50, Validation Loss: 7.892404556274414, Accuracy: 0.5210843086242676\n",
            "Epoch 9/50, Validation Loss: 7.891769886016846, Accuracy: 0.5240963697433472\n",
            "Epoch 10/50, Validation Loss: 7.8938398361206055, Accuracy: 0.5271084308624268\n",
            "Epoch 11/50, Validation Loss: 7.897712230682373, Accuracy: 0.5301204919815063\n",
            "Epoch 12/50, Validation Loss: 7.902705669403076, Accuracy: 0.5271084308624268\n",
            "Epoch 13/50, Validation Loss: 7.908337593078613, Accuracy: 0.5180723071098328\n",
            "Epoch 14/50, Validation Loss: 7.914272308349609, Accuracy: 0.5150602459907532\n",
            "Epoch 15/50, Validation Loss: 7.920289039611816, Accuracy: 0.5120481848716736\n",
            "Epoch 16/50, Validation Loss: 7.926246166229248, Accuracy: 0.5120481848716736\n",
            "Epoch 17/50, Validation Loss: 7.932058334350586, Accuracy: 0.5120481848716736\n",
            "Epoch 18/50, Validation Loss: 7.937680721282959, Accuracy: 0.509036123752594\n",
            "Epoch 19/50, Validation Loss: 7.943091869354248, Accuracy: 0.5060241222381592\n",
            "Epoch 20/50, Validation Loss: 7.94828462600708, Accuracy: 0.5060241222381592\n",
            "Epoch 21/50, Validation Loss: 7.953260898590088, Accuracy: 0.5060241222381592\n",
            "Epoch 22/50, Validation Loss: 7.95802640914917, Accuracy: 0.509036123752594\n",
            "Epoch 23/50, Validation Loss: 7.962581634521484, Accuracy: 0.5150602459907532\n",
            "Epoch 24/50, Validation Loss: 7.966935157775879, Accuracy: 0.509036123752594\n",
            "Epoch 25/50, Validation Loss: 7.971088409423828, Accuracy: 0.509036123752594\n",
            "Epoch 26/50, Validation Loss: 7.9750471115112305, Accuracy: 0.509036123752594\n",
            "Epoch 27/50, Validation Loss: 7.978816509246826, Accuracy: 0.5060241222381592\n",
            "Epoch 28/50, Validation Loss: 7.9824042320251465, Accuracy: 0.5030120611190796\n",
            "Epoch 29/50, Validation Loss: 7.985815048217773, Accuracy: 0.5060241222381592\n",
            "Epoch 30/50, Validation Loss: 7.989058017730713, Accuracy: 0.5060241222381592\n",
            "Epoch 31/50, Validation Loss: 7.992141246795654, Accuracy: 0.509036123752594\n",
            "Epoch 32/50, Validation Loss: 7.9950714111328125, Accuracy: 0.5120481848716736\n",
            "Epoch 33/50, Validation Loss: 7.997859477996826, Accuracy: 0.5120481848716736\n",
            "Epoch 34/50, Validation Loss: 8.000510215759277, Accuracy: 0.5\n",
            "Epoch 35/50, Validation Loss: 8.003035545349121, Accuracy: 0.5030120611190796\n",
            "Epoch 36/50, Validation Loss: 8.005441665649414, Accuracy: 0.4969879388809204\n",
            "Epoch 37/50, Validation Loss: 8.007735252380371, Accuracy: 0.4969879388809204\n",
            "Epoch 38/50, Validation Loss: 8.00992488861084, Accuracy: 0.4969879388809204\n",
            "Epoch 39/50, Validation Loss: 8.012019157409668, Accuracy: 0.4969879388809204\n",
            "Epoch 40/50, Validation Loss: 8.014023780822754, Accuracy: 0.5\n",
            "Epoch 41/50, Validation Loss: 8.015942573547363, Accuracy: 0.4909638464450836\n",
            "Epoch 42/50, Validation Loss: 8.01778793334961, Accuracy: 0.4879518151283264\n",
            "Epoch 43/50, Validation Loss: 8.019558906555176, Accuracy: 0.4909638464450836\n",
            "Epoch 44/50, Validation Loss: 8.021267890930176, Accuracy: 0.4909638464450836\n",
            "Epoch 45/50, Validation Loss: 8.02291202545166, Accuracy: 0.4939759075641632\n",
            "Epoch 46/50, Validation Loss: 8.024502754211426, Accuracy: 0.4939759075641632\n",
            "Epoch 47/50, Validation Loss: 8.026041030883789, Accuracy: 0.4969879388809204\n",
            "Epoch 48/50, Validation Loss: 8.027528762817383, Accuracy: 0.4969879388809204\n",
            "Epoch 49/50, Validation Loss: 8.028975486755371, Accuracy: 0.5060241222381592\n",
            "Epoch 50/50, Validation Loss: 8.030380249023438, Accuracy: 0.5120481848716736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = (predictions > 0.2).astype(int)"
      ],
      "metadata": {
        "id": "-PlMJC__Ip1q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "\n",
        "accuracy = accuracy_score(tttttttt.tes_gt, predicted_labels)\n",
        "precision = precision_score(tttttttt.tes_gt, predicted_labels)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYEc_Wo7Isgf",
        "outputId": "3fb4adb2-271f-4ae7-a109-2429c3b3161d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.48500881834215165\n",
            "Precision: 0.5399449035812672\n"
          ]
        }
      ]
    }
  ]
}